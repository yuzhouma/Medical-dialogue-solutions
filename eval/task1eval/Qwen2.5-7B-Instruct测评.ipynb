{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5447e827-bd21-4d7f-8230-7d651b2f3f8d",
   "metadata": {},
   "source": [
    "# CMMLU测评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d103c6cf-1ce2-4535-beca-8f7518d6dfa1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-09-29T04:55:34.663264Z",
     "iopub.status.busy": "2025-09-29T04:55:34.663058Z",
     "iopub.status.idle": "2025-09-29T05:08:50.291262Z",
     "shell.execute_reply": "2025-09-29T05:08:50.290822Z",
     "shell.execute_reply.started": "2025-09-29T04:55:34.663248Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'开始timestamp': '2025-09-29 12:55:35'}\n",
      "总请求数: 11582, 线程池大小: 200\n",
      "\n",
      "处理批次: 1 - 200/11582\n",
      "批次 1-200 结果已保存\n",
      "\n",
      "处理批次: 201 - 400/11582\n",
      "批次 201-400 结果已保存\n",
      "\n",
      "处理批次: 401 - 600/11582\n",
      "批次 401-600 结果已保存\n",
      "\n",
      "处理批次: 601 - 800/11582\n",
      "批次 601-800 结果已保存\n",
      "\n",
      "处理批次: 801 - 1000/11582\n",
      "批次 801-1000 结果已保存\n",
      "\n",
      "处理批次: 1001 - 1200/11582\n",
      "批次 1001-1200 结果已保存\n",
      "\n",
      "处理批次: 1201 - 1400/11582\n",
      "批次 1201-1400 结果已保存\n",
      "\n",
      "处理批次: 1401 - 1600/11582\n",
      "批次 1401-1600 结果已保存\n",
      "\n",
      "处理批次: 1601 - 1800/11582\n",
      "批次 1601-1800 结果已保存\n",
      "\n",
      "处理批次: 1801 - 2000/11582\n",
      "批次 1801-2000 结果已保存\n",
      "\n",
      "处理批次: 2001 - 2200/11582\n",
      "批次 2001-2200 结果已保存\n",
      "\n",
      "处理批次: 2201 - 2400/11582\n",
      "批次 2201-2400 结果已保存\n",
      "\n",
      "处理批次: 2401 - 2600/11582\n",
      "批次 2401-2600 结果已保存\n",
      "\n",
      "处理批次: 2601 - 2800/11582\n",
      "批次 2601-2800 结果已保存\n",
      "\n",
      "处理批次: 2801 - 3000/11582\n",
      "批次 2801-3000 结果已保存\n",
      "\n",
      "处理批次: 3001 - 3200/11582\n",
      "批次 3001-3200 结果已保存\n",
      "\n",
      "处理批次: 3201 - 3400/11582\n",
      "批次 3201-3400 结果已保存\n",
      "\n",
      "处理批次: 3401 - 3600/11582\n",
      "批次 3401-3600 结果已保存\n",
      "\n",
      "处理批次: 3601 - 3800/11582\n",
      "批次 3601-3800 结果已保存\n",
      "\n",
      "处理批次: 3801 - 4000/11582\n",
      "批次 3801-4000 结果已保存\n",
      "\n",
      "处理批次: 4001 - 4200/11582\n",
      "批次 4001-4200 结果已保存\n",
      "\n",
      "处理批次: 4201 - 4400/11582\n",
      "批次 4201-4400 结果已保存\n",
      "\n",
      "处理批次: 4401 - 4600/11582\n",
      "批次 4401-4600 结果已保存\n",
      "\n",
      "处理批次: 4601 - 4800/11582\n",
      "批次 4601-4800 结果已保存\n",
      "\n",
      "处理批次: 4801 - 5000/11582\n",
      "批次 4801-5000 结果已保存\n",
      "\n",
      "处理批次: 5001 - 5200/11582\n",
      "批次 5001-5200 结果已保存\n",
      "\n",
      "处理批次: 5201 - 5400/11582\n",
      "批次 5201-5400 结果已保存\n",
      "\n",
      "处理批次: 5401 - 5600/11582\n",
      "批次 5401-5600 结果已保存\n",
      "\n",
      "处理批次: 5601 - 5800/11582\n",
      "批次 5601-5800 结果已保存\n",
      "\n",
      "处理批次: 5801 - 6000/11582\n",
      "批次 5801-6000 结果已保存\n",
      "\n",
      "处理批次: 6001 - 6200/11582\n",
      "批次 6001-6200 结果已保存\n",
      "\n",
      "处理批次: 6201 - 6400/11582\n",
      "批次 6201-6400 结果已保存\n",
      "\n",
      "处理批次: 6401 - 6600/11582\n",
      "批次 6401-6600 结果已保存\n",
      "\n",
      "处理批次: 6601 - 6800/11582\n",
      "批次 6601-6800 结果已保存\n",
      "\n",
      "处理批次: 6801 - 7000/11582\n",
      "批次 6801-7000 结果已保存\n",
      "\n",
      "处理批次: 7001 - 7200/11582\n",
      "批次 7001-7200 结果已保存\n",
      "\n",
      "处理批次: 7201 - 7400/11582\n",
      "批次 7201-7400 结果已保存\n",
      "\n",
      "处理批次: 7401 - 7600/11582\n",
      "批次 7401-7600 结果已保存\n",
      "\n",
      "处理批次: 7601 - 7800/11582\n",
      "批次 7601-7800 结果已保存\n",
      "\n",
      "处理批次: 7801 - 8000/11582\n",
      "批次 7801-8000 结果已保存\n",
      "\n",
      "处理批次: 8001 - 8200/11582\n",
      "批次 8001-8200 结果已保存\n",
      "\n",
      "处理批次: 8201 - 8400/11582\n",
      "批次 8201-8400 结果已保存\n",
      "\n",
      "处理批次: 8401 - 8600/11582\n",
      "批次 8401-8600 结果已保存\n",
      "\n",
      "处理批次: 8601 - 8800/11582\n",
      "批次 8601-8800 结果已保存\n",
      "\n",
      "处理批次: 8801 - 9000/11582\n",
      "批次 8801-9000 结果已保存\n",
      "\n",
      "处理批次: 9001 - 9200/11582\n",
      "批次 9001-9200 结果已保存\n",
      "\n",
      "处理批次: 9201 - 9400/11582\n",
      "批次 9201-9400 结果已保存\n",
      "\n",
      "处理批次: 9401 - 9600/11582\n",
      "批次 9401-9600 结果已保存\n",
      "\n",
      "处理批次: 9601 - 9800/11582\n",
      "批次 9601-9800 结果已保存\n",
      "\n",
      "处理批次: 9801 - 10000/11582\n",
      "批次 9801-10000 结果已保存\n",
      "\n",
      "处理批次: 10001 - 10200/11582\n",
      "批次 10001-10200 结果已保存\n",
      "\n",
      "处理批次: 10201 - 10400/11582\n",
      "批次 10201-10400 结果已保存\n",
      "\n",
      "处理批次: 10401 - 10600/11582\n",
      "批次 10401-10600 结果已保存\n",
      "\n",
      "处理批次: 10601 - 10800/11582\n",
      "批次 10601-10800 结果已保存\n",
      "\n",
      "处理批次: 10801 - 11000/11582\n",
      "批次 10801-11000 结果已保存\n",
      "\n",
      "处理批次: 11001 - 11200/11582\n",
      "批次 11001-11200 结果已保存\n",
      "\n",
      "处理批次: 11201 - 11400/11582\n",
      "批次 11201-11400 结果已保存\n",
      "\n",
      "处理批次: 11401 - 11582/11582\n",
      "批次 11401-11582 结果已保存\n",
      "\n",
      "所有请求处理完成，最终结果已保存到 ./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-cmmlu_result.json\n",
      "{'结束timestamp': '2025-09-29 13:08:49'}\n",
      "处理完成！共处理 11582 条数据，生成 67 个统计结果\n",
      "{'统计结果已保存': '2025-09-29 13:08:50'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 swift deploy \\\n",
    "    --model Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --infer_backend vllm \\\n",
    "    --max_new_tokens 2048 \\\n",
    "    --served_model_name Qwen2.5-7B-Instruct\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "import sys\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests  \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def call_llm_api(prompt, api_url):\n",
    "    \"\"\"调用大模型API\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"Qwen2.5-7B-Instruct\",\n",
    "        # \"model\": \"lora\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        # 从聊天接口正确解析结果（chat completions返回的是message而非text）\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"API调用错误: {str(e)}\")\n",
    "        # 抛出异常而非返回错误字符串，让上层处理\n",
    "        raise Exception(f\"API调用错误: {str(e)}\")\n",
    "\n",
    "def process_in_batches(data, api_url, output_file=\"./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-cmmlu_result.json\"):\n",
    "    total = len(data)\n",
    "    print(f\"总请求数: {total}, 线程池大小: {MAX_WORKERS}\")\n",
    "\n",
    "    results = []\n",
    "    lock = threading.Lock()  # 确保多线程安全写入结果列表\n",
    "\n",
    "    # 初始化线程池\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 分批次处理\n",
    "        for batch_start in range(0, total, MAX_WORKERS):\n",
    "#             if batch_start%400 ==0 or batch_start%300==0:\n",
    "\n",
    "            batch_end = min(batch_start + MAX_WORKERS, total)\n",
    "            print(f\"\\n处理批次: {batch_start+1} - {batch_end}/{total}\")\n",
    "\n",
    "            # 提交当前批次的所有任务到线程池\n",
    "            futures = {}\n",
    "            for i in range(batch_start, batch_end):\n",
    "                item = data[i]['question_with_options']\n",
    "\n",
    "                # 修复：确保content变量已定义（这里假设content是你的prompt模板）\n",
    "                # 如果你没有定义content，请替换为实际的prompt构建逻辑\n",
    "                prompt = content+item\n",
    "                future = executor.submit(\n",
    "                    call_llm_api,\n",
    "                    prompt,  # 使用构建好的prompt\n",
    "                    api_url\n",
    "                )\n",
    "                futures[future] = (i, data[i])  # 关联future与请求索引和数据\n",
    "\n",
    "            # 等待当前批次所有任务完成\n",
    "            batch_results = []\n",
    "            for future in as_completed(futures):\n",
    "                i, item = futures[future]\n",
    "                try:\n",
    "                    model_response = future.result()\n",
    "                    batch_results.append({\n",
    "                        \"question_with_options\": item['question_with_options'],\n",
    "                        \"answer\": item['answer'],\n",
    "                        \"source_file\":item['source_file'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                    # print(f\"完成请求 #{i+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"请求 #{i+1} 处理失败: {str(e)}\")\n",
    "                    batch_results.append({\n",
    "                        \"error\": str(e),\n",
    "                        \"question_with_options\": item['question_with_options'],\n",
    "                        \"answer\": item['answer'],\n",
    "                        \"source_file\":item['source_file'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"failed\"\n",
    "                    })\n",
    "                \n",
    "            # 批次完成后合并结果并保存\n",
    "            with lock:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            # 每批处理完成后保存一次结果\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"批次 {batch_start+1}-{batch_end} 结果已保存\")\n",
    "\n",
    "    print(f\"\\n所有请求处理完成，最终结果已保存到 {output_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_json_file(input_file, output_file='./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-eval_results.xlsx'):\n",
    "    \"\"\"\n",
    "    直接处理JSON文件并生成结果\n",
    "    \"\"\"\n",
    "    # 读取JSON文件\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 按source_file分组统计\n",
    "    stats = {}\n",
    "    for item in data:\n",
    "        source_file = item['source_file']\n",
    "        if source_file not in stats:\n",
    "            stats[source_file] = {'correct': 0, 'total': 0}\n",
    "        \n",
    "        stats[source_file]['total'] += 1\n",
    "        if item['answer'] == item['model_response']:\n",
    "            stats[source_file]['correct'] += 1\n",
    "    \n",
    "    # 准备结果数据\n",
    "    results = []\n",
    "    for source_file, stat in stats.items():\n",
    "        acc = stat['correct'] / stat['total'] if stat['total'] > 0 else 0\n",
    "        results.append({\n",
    "\n",
    "            \n",
    "            '源文件': source_file,\n",
    "            '正确数': stat['correct'],\n",
    "            '总数': stat['total'], \n",
    "            '准确率': round(acc, 4)\n",
    "        })\n",
    "    \n",
    "    # 保存到Excel\n",
    "    df = pd.DataFrame(results)\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='cmmlu各个子集acc结果', index=False)\n",
    "    \n",
    "    print(f\"处理完成！共处理 {len(data)} 条数据，生成 {len(results)} 个统计结果\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cmmlu_eval_df = pd.read_csv('cmmlu_concat.csv')\n",
    "    cmmlu_eval_json = cmmlu_eval_df.to_json(orient='records', force_ascii=False, indent=2)\n",
    "    cmmlu_eval_json = json.loads(cmmlu_eval_json)\n",
    "\n",
    "    with open('../../prompt/CMMLU评估', 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "\n",
    "\n",
    "    # 配置参数\n",
    "    MAX_WORKERS = 200 # 线程池最大线程数，同时也是每批处理的请求数\n",
    "    API_URL = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "    print({\"开始timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_in_batches(cmmlu_eval_json, API_URL)\n",
    "    print({\"结束timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_json_file('./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-cmmlu_result.json')\n",
    "    print({\"统计结果已保存\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbae9b-919a-4cf3-9e29-9c641dbe0f01",
   "metadata": {},
   "source": [
    "# 测试集测评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19902fc4-9c0b-492f-9d15-3a141811304b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-09-29T07:46:41.364663Z",
     "iopub.status.busy": "2025-09-29T07:46:41.364459Z",
     "iopub.status.idle": "2025-09-29T08:12:18.411091Z",
     "shell.execute_reply": "2025-09-29T08:12:18.410548Z",
     "shell.execute_reply.started": "2025-09-29T07:46:41.364648Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'开始timestamp': '2025-09-29 15:46:41'}\n",
      "总请求数: 4671, 线程池大小: 100\n",
      "\n",
      "处理批次: 1 - 100/4671\n",
      "批次 1-100 结果已保存\n",
      "\n",
      "处理批次: 101 - 200/4671\n",
      "批次 101-200 结果已保存\n",
      "\n",
      "处理批次: 201 - 300/4671\n",
      "批次 201-300 结果已保存\n",
      "\n",
      "处理批次: 301 - 400/4671\n",
      "批次 301-400 结果已保存\n",
      "\n",
      "处理批次: 401 - 500/4671\n",
      "批次 401-500 结果已保存\n",
      "\n",
      "处理批次: 501 - 600/4671\n",
      "批次 501-600 结果已保存\n",
      "\n",
      "处理批次: 601 - 700/4671\n",
      "批次 601-700 结果已保存\n",
      "\n",
      "处理批次: 701 - 800/4671\n",
      "批次 701-800 结果已保存\n",
      "\n",
      "处理批次: 801 - 900/4671\n",
      "批次 801-900 结果已保存\n",
      "\n",
      "处理批次: 901 - 1000/4671\n",
      "批次 901-1000 结果已保存\n",
      "\n",
      "处理批次: 1001 - 1100/4671\n",
      "批次 1001-1100 结果已保存\n",
      "\n",
      "处理批次: 1101 - 1200/4671\n",
      "批次 1101-1200 结果已保存\n",
      "\n",
      "处理批次: 1201 - 1300/4671\n",
      "批次 1201-1300 结果已保存\n",
      "\n",
      "处理批次: 1301 - 1400/4671\n",
      "批次 1301-1400 结果已保存\n",
      "\n",
      "处理批次: 1401 - 1500/4671\n",
      "批次 1401-1500 结果已保存\n",
      "\n",
      "处理批次: 1501 - 1600/4671\n",
      "批次 1501-1600 结果已保存\n",
      "\n",
      "处理批次: 1601 - 1700/4671\n",
      "批次 1601-1700 结果已保存\n",
      "\n",
      "处理批次: 1701 - 1800/4671\n",
      "批次 1701-1800 结果已保存\n",
      "\n",
      "处理批次: 1801 - 1900/4671\n",
      "批次 1801-1900 结果已保存\n",
      "\n",
      "处理批次: 1901 - 2000/4671\n",
      "批次 1901-2000 结果已保存\n",
      "\n",
      "处理批次: 2001 - 2100/4671\n",
      "批次 2001-2100 结果已保存\n",
      "\n",
      "处理批次: 2101 - 2200/4671\n",
      "批次 2101-2200 结果已保存\n",
      "\n",
      "处理批次: 2201 - 2300/4671\n",
      "批次 2201-2300 结果已保存\n",
      "\n",
      "处理批次: 2301 - 2400/4671\n",
      "批次 2301-2400 结果已保存\n",
      "\n",
      "处理批次: 2401 - 2500/4671\n",
      "批次 2401-2500 结果已保存\n",
      "\n",
      "处理批次: 2501 - 2600/4671\n",
      "批次 2501-2600 结果已保存\n",
      "\n",
      "处理批次: 2601 - 2700/4671\n",
      "批次 2601-2700 结果已保存\n",
      "\n",
      "处理批次: 2701 - 2800/4671\n",
      "批次 2701-2800 结果已保存\n",
      "\n",
      "处理批次: 2801 - 2900/4671\n",
      "批次 2801-2900 结果已保存\n",
      "\n",
      "处理批次: 2901 - 3000/4671\n",
      "批次 2901-3000 结果已保存\n",
      "\n",
      "处理批次: 3001 - 3100/4671\n",
      "批次 3001-3100 结果已保存\n",
      "\n",
      "处理批次: 3101 - 3200/4671\n",
      "批次 3101-3200 结果已保存\n",
      "\n",
      "处理批次: 3201 - 3300/4671\n",
      "批次 3201-3300 结果已保存\n",
      "\n",
      "处理批次: 3301 - 3400/4671\n",
      "批次 3301-3400 结果已保存\n",
      "\n",
      "处理批次: 3401 - 3500/4671\n",
      "批次 3401-3500 结果已保存\n",
      "\n",
      "处理批次: 3501 - 3600/4671\n",
      "批次 3501-3600 结果已保存\n",
      "\n",
      "处理批次: 3601 - 3700/4671\n",
      "批次 3601-3700 结果已保存\n",
      "\n",
      "处理批次: 3701 - 3800/4671\n",
      "批次 3701-3800 结果已保存\n",
      "\n",
      "处理批次: 3801 - 3900/4671\n",
      "批次 3801-3900 结果已保存\n",
      "\n",
      "处理批次: 3901 - 4000/4671\n",
      "批次 3901-4000 结果已保存\n",
      "\n",
      "处理批次: 4001 - 4100/4671\n",
      "批次 4001-4100 结果已保存\n",
      "\n",
      "处理批次: 4101 - 4200/4671\n",
      "批次 4101-4200 结果已保存\n",
      "\n",
      "处理批次: 4201 - 4300/4671\n",
      "批次 4201-4300 结果已保存\n",
      "\n",
      "处理批次: 4301 - 4400/4671\n",
      "批次 4301-4400 结果已保存\n",
      "\n",
      "处理批次: 4401 - 4500/4671\n",
      "批次 4401-4500 结果已保存\n",
      "\n",
      "处理批次: 4501 - 4600/4671\n",
      "批次 4501-4600 结果已保存\n",
      "\n",
      "处理批次: 4601 - 4671/4671\n",
      "批次 4601-4671 结果已保存\n",
      "\n",
      "所有请求处理完成，最终结果已保存到 ./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-test_result.json\n",
      "{'结束timestamp': '2025-09-29 16:12:18'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 swift deploy \\\n",
    "    --model Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --infer_backend vllm \\\n",
    "    --max_new_tokens 2048 \\\n",
    "    --served_model_name Qwen2.5-7B-Instruct\n",
    "'''\n",
    "content = '现在你是一个肿瘤学科医生，请根据患者的问题给出实际的医疗建议：'\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests  \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def call_llm_api(prompt, api_url):\n",
    "    \"\"\"调用大模型API\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"Qwen2.5-7B-Instruct\",\n",
    "        # \"model\": \"lora\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        # 从聊天接口正确解析结果（chat completions返回的是message而非text）\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"API调用错误: {str(e)}\")\n",
    "        # 抛出异常而非返回错误字符串，让上层处理\n",
    "        raise Exception(f\"API调用错误: {str(e)}\")\n",
    "\n",
    "def process_in_batches(data, api_url, output_file=\"./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-test_result.json\"):\n",
    "    total = len(data)\n",
    "    print(f\"总请求数: {total}, 线程池大小: {MAX_WORKERS}\")\n",
    "\n",
    "    results = []\n",
    "    lock = threading.Lock()  # 确保多线程安全写入结果列表\n",
    "\n",
    "    # 初始化线程池\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 分批次处理\n",
    "        for batch_start in range(0, total, MAX_WORKERS):\n",
    "#             if batch_start%400 ==0 or batch_start%300==0:\n",
    "\n",
    "            batch_end = min(batch_start + MAX_WORKERS, total)\n",
    "            print(f\"\\n处理批次: {batch_start+1} - {batch_end}/{total}\")\n",
    "\n",
    "            # 提交当前批次的所有任务到线程池\n",
    "            futures = {}\n",
    "            for i in range(batch_start, batch_end):\n",
    "                item = data[i]['input']\n",
    "\n",
    "                # 修复：确保content变量已定义（这里假设content是你的prompt模板）\n",
    "                # 如果你没有定义content，请替换为实际的prompt构建逻辑\n",
    "                prompt = content+item\n",
    "                future = executor.submit(\n",
    "                    call_llm_api,\n",
    "                    prompt,  # 使用构建好的prompt\n",
    "                    api_url\n",
    "                )\n",
    "                futures[future] = (i, data[i])  # 关联future与请求索引和数据\n",
    "\n",
    "            # 等待当前批次所有任务完成\n",
    "            batch_results = []\n",
    "            for future in as_completed(futures):\n",
    "                i, item = futures[future]\n",
    "                try:\n",
    "                    model_response = future.result()\n",
    "                    batch_results.append({\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": item['output'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                    # print(f\"完成请求 #{i+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"请求 #{i+1} 处理失败: {str(e)}\")\n",
    "                    batch_results.append({\n",
    "                        \"error\": str(e),\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": item['output'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"failed\"\n",
    "                    })\n",
    "                \n",
    "            # 批次完成后合并结果并保存\n",
    "            with lock:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            # 每批处理完成后保存一次结果\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"批次 {batch_start+1}-{batch_end} 结果已保存\")\n",
    "\n",
    "    print(f\"\\n所有请求处理完成，最终结果已保存到 {output_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('../../data_process/final_data/test.json', 'r', encoding='utf-8') as file:\n",
    "        test_json = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "    # 配置参数\n",
    "    MAX_WORKERS = 100 # 线程池最大线程数，同时也是每批处理的请求数\n",
    "    API_URL = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "    print({\"开始timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_in_batches(test_json, API_URL)\n",
    "    print({\"结束timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aeca1778",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
