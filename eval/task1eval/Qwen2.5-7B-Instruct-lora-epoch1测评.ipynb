{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d103c6cf-1ce2-4535-beca-8f7518d6dfa1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-09-29T09:57:50.658479Z",
     "iopub.status.busy": "2025-09-29T09:57:50.658260Z",
     "iopub.status.idle": "2025-09-29T10:05:17.223017Z",
     "shell.execute_reply": "2025-09-29T10:05:17.222591Z",
     "shell.execute_reply.started": "2025-09-29T09:57:50.658463Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'开始timestamp': '2025-09-29 17:57:50'}\n",
      "总请求数: 11582, 线程池大小: 250\n",
      "\n",
      "处理批次: 1 - 250/11582\n",
      "批次 1-250 结果已保存\n",
      "\n",
      "处理批次: 251 - 500/11582\n",
      "批次 251-500 结果已保存\n",
      "\n",
      "处理批次: 501 - 750/11582\n",
      "批次 501-750 结果已保存\n",
      "\n",
      "处理批次: 751 - 1000/11582\n",
      "批次 751-1000 结果已保存\n",
      "\n",
      "处理批次: 1001 - 1250/11582\n",
      "批次 1001-1250 结果已保存\n",
      "\n",
      "处理批次: 1251 - 1500/11582\n",
      "批次 1251-1500 结果已保存\n",
      "\n",
      "处理批次: 1501 - 1750/11582\n",
      "批次 1501-1750 结果已保存\n",
      "\n",
      "处理批次: 1751 - 2000/11582\n",
      "批次 1751-2000 结果已保存\n",
      "\n",
      "处理批次: 2001 - 2250/11582\n",
      "批次 2001-2250 结果已保存\n",
      "\n",
      "处理批次: 2251 - 2500/11582\n",
      "批次 2251-2500 结果已保存\n",
      "\n",
      "处理批次: 2501 - 2750/11582\n",
      "批次 2501-2750 结果已保存\n",
      "\n",
      "处理批次: 2751 - 3000/11582\n",
      "批次 2751-3000 结果已保存\n",
      "\n",
      "处理批次: 3001 - 3250/11582\n",
      "批次 3001-3250 结果已保存\n",
      "\n",
      "处理批次: 3251 - 3500/11582\n",
      "批次 3251-3500 结果已保存\n",
      "\n",
      "处理批次: 3501 - 3750/11582\n",
      "批次 3501-3750 结果已保存\n",
      "\n",
      "处理批次: 3751 - 4000/11582\n",
      "批次 3751-4000 结果已保存\n",
      "\n",
      "处理批次: 4001 - 4250/11582\n",
      "批次 4001-4250 结果已保存\n",
      "\n",
      "处理批次: 4251 - 4500/11582\n",
      "批次 4251-4500 结果已保存\n",
      "\n",
      "处理批次: 4501 - 4750/11582\n",
      "批次 4501-4750 结果已保存\n",
      "\n",
      "处理批次: 4751 - 5000/11582\n",
      "批次 4751-5000 结果已保存\n",
      "\n",
      "处理批次: 5001 - 5250/11582\n",
      "批次 5001-5250 结果已保存\n",
      "\n",
      "处理批次: 5251 - 5500/11582\n",
      "批次 5251-5500 结果已保存\n",
      "\n",
      "处理批次: 5501 - 5750/11582\n",
      "批次 5501-5750 结果已保存\n",
      "\n",
      "处理批次: 5751 - 6000/11582\n",
      "批次 5751-6000 结果已保存\n",
      "\n",
      "处理批次: 6001 - 6250/11582\n",
      "批次 6001-6250 结果已保存\n",
      "\n",
      "处理批次: 6251 - 6500/11582\n",
      "批次 6251-6500 结果已保存\n",
      "\n",
      "处理批次: 6501 - 6750/11582\n",
      "批次 6501-6750 结果已保存\n",
      "\n",
      "处理批次: 6751 - 7000/11582\n",
      "批次 6751-7000 结果已保存\n",
      "\n",
      "处理批次: 7001 - 7250/11582\n",
      "批次 7001-7250 结果已保存\n",
      "\n",
      "处理批次: 7251 - 7500/11582\n",
      "批次 7251-7500 结果已保存\n",
      "\n",
      "处理批次: 7501 - 7750/11582\n",
      "批次 7501-7750 结果已保存\n",
      "\n",
      "处理批次: 7751 - 8000/11582\n",
      "批次 7751-8000 结果已保存\n",
      "\n",
      "处理批次: 8001 - 8250/11582\n",
      "批次 8001-8250 结果已保存\n",
      "\n",
      "处理批次: 8251 - 8500/11582\n",
      "批次 8251-8500 结果已保存\n",
      "\n",
      "处理批次: 8501 - 8750/11582\n",
      "批次 8501-8750 结果已保存\n",
      "\n",
      "处理批次: 8751 - 9000/11582\n",
      "批次 8751-9000 结果已保存\n",
      "\n",
      "处理批次: 9001 - 9250/11582\n",
      "批次 9001-9250 结果已保存\n",
      "\n",
      "处理批次: 9251 - 9500/11582\n",
      "批次 9251-9500 结果已保存\n",
      "\n",
      "处理批次: 9501 - 9750/11582\n",
      "批次 9501-9750 结果已保存\n",
      "\n",
      "处理批次: 9751 - 10000/11582\n",
      "批次 9751-10000 结果已保存\n",
      "\n",
      "处理批次: 10001 - 10250/11582\n",
      "批次 10001-10250 结果已保存\n",
      "\n",
      "处理批次: 10251 - 10500/11582\n",
      "批次 10251-10500 结果已保存\n",
      "\n",
      "处理批次: 10501 - 10750/11582\n",
      "批次 10501-10750 结果已保存\n",
      "\n",
      "处理批次: 10751 - 11000/11582\n",
      "批次 10751-11000 结果已保存\n",
      "\n",
      "处理批次: 11001 - 11250/11582\n",
      "批次 11001-11250 结果已保存\n",
      "\n",
      "处理批次: 11251 - 11500/11582\n",
      "批次 11251-11500 结果已保存\n",
      "\n",
      "处理批次: 11501 - 11582/11582\n",
      "批次 11501-11582 结果已保存\n",
      "\n",
      "所有请求处理完成，最终结果已保存到 ./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-cmmlu_result.json\n",
      "{'结束timestamp': '2025-09-29 18:05:16'}\n",
      "处理完成！共处理 11582 条数据，生成 67 个统计结果\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift deploy \\\n",
    "    --adapters lora=./train_ada/v3-20250928-095853/checkpoint-2300 \\\n",
    "    --infer_backend vllm \\\n",
    "    --temperature 0 \\\n",
    "    --max_new_tokens 2048 \\\n",
    "    --served_model_name Qwen2.5-7B-Instruct-lora1\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import sys\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests  # 需安装：pip install requests\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def call_llm_api(prompt, api_url):\n",
    "    \"\"\"调用大模型API\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        # \"model\": \"Qwen2.5-7B-Instruct\",\n",
    "        \"model\": \"lora\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        # 从聊天接口正确解析结果（chat completions返回的是message而非text）\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"API调用错误: {str(e)}\")\n",
    "        # 抛出异常而非返回错误字符串，让上层处理\n",
    "        raise Exception(f\"API调用错误: {str(e)}\")\n",
    "\n",
    "def process_in_batches(data, api_url, output_file=\"./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-cmmlu_result.json\"):\n",
    "    total = len(data)\n",
    "    print(f\"总请求数: {total}, 线程池大小: {MAX_WORKERS}\")\n",
    "\n",
    "    results = []\n",
    "    lock = threading.Lock()  # 确保多线程安全写入结果列表\n",
    "\n",
    "    # 初始化线程池\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 分批次处理\n",
    "        for batch_start in range(0, total, MAX_WORKERS):\n",
    "#             if batch_start%400 ==0 or batch_start%300==0:\n",
    "\n",
    "            batch_end = min(batch_start + MAX_WORKERS, total)\n",
    "            print(f\"\\n处理批次: {batch_start+1} - {batch_end}/{total}\")\n",
    "\n",
    "            # 提交当前批次的所有任务到线程池\n",
    "            futures = {}\n",
    "            for i in range(batch_start, batch_end):\n",
    "                item = data[i]['question_with_options']\n",
    "\n",
    "                # 修复：确保content变量已定义（这里假设content是你的prompt模板）\n",
    "                # 如果你没有定义content，请替换为实际的prompt构建逻辑\n",
    "                prompt = content+item\n",
    "                future = executor.submit(\n",
    "                    call_llm_api,\n",
    "                    prompt,  # 使用构建好的prompt\n",
    "                    api_url\n",
    "                )\n",
    "                futures[future] = (i, data[i])  # 关联future与请求索引和数据\n",
    "\n",
    "            # 等待当前批次所有任务完成\n",
    "            batch_results = []\n",
    "            for future in as_completed(futures):\n",
    "                i, item = futures[future]\n",
    "                try:\n",
    "                    model_response = future.result()\n",
    "                    batch_results.append({\n",
    "                        \"question_with_options\": item['question_with_options'],\n",
    "                        \"answer\": item['answer'],\n",
    "                        \"source_file\":item['source_file'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                    # print(f\"完成请求 #{i+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"请求 #{i+1} 处理失败: {str(e)}\")\n",
    "                    batch_results.append({\n",
    "                        \"error\": str(e),\n",
    "                        \"question_with_options\": item['question_with_options'],\n",
    "                        \"answer\": item['answer'],\n",
    "                        \"source_file\":item['source_file'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"failed\"\n",
    "                    })\n",
    "                \n",
    "            # 批次完成后合并结果并保存\n",
    "            with lock:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            # 每批处理完成后保存一次结果\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"批次 {batch_start+1}-{batch_end} 结果已保存\")\n",
    "\n",
    "    print(f\"\\n所有请求处理完成，最终结果已保存到 {output_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_json_file(input_file, output_file='./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-eval_results.xlsx'):\n",
    "    \"\"\"\n",
    "    直接处理JSON文件并生成结果\n",
    "    \"\"\"\n",
    "    # 读取JSON文件\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 按source_file分组统计\n",
    "    stats = {}\n",
    "    for item in data:\n",
    "        source_file = item['source_file']\n",
    "        if source_file not in stats:\n",
    "            stats[source_file] = {'correct': 0, 'total': 0}\n",
    "        \n",
    "        stats[source_file]['total'] += 1\n",
    "        if item['answer'] == item['model_response']:\n",
    "            stats[source_file]['correct'] += 1\n",
    "    \n",
    "    # 准备结果数据\n",
    "    results = []\n",
    "    for source_file, stat in stats.items():\n",
    "        acc = stat['correct'] / stat['total'] if stat['total'] > 0 else 0\n",
    "        results.append({\n",
    "            '源文件': source_file,\n",
    "            '正确数': stat['correct'],\n",
    "            '总数': stat['total'], \n",
    "            '准确率': round(acc, 4)\n",
    "        })\n",
    "    \n",
    "    # 保存到Excel\n",
    "    df = pd.DataFrame(results)\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='cmmlu各个子集acc结果', index=False)\n",
    "    \n",
    "    print(f\"处理完成！共处理 {len(data)} 条数据，生成 {len(results)} 个统计结果\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cmmlu_eval_df = pd.read_csv('cmmlu_concat.csv')\n",
    "    cmmlu_eval_json = cmmlu_eval_df.to_json(orient='records', force_ascii=False, indent=2)\n",
    "    cmmlu_eval_json = json.loads(cmmlu_eval_json)\n",
    "\n",
    "    with open('../../prompt/CMMLU评估', 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "\n",
    "\n",
    "    # 配置参数\n",
    "    MAX_WORKERS = 250 # 线程池最大线程数，同时也是每批处理的请求数\n",
    "    API_URL = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "    print({\"开始timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_in_batches(cmmlu_eval_json, API_URL)\n",
    "    print({\"结束timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_json_file('./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-cmmlu_result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "19902fc4-9c0b-492f-9d15-3a141811304b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2025-09-29T10:05:17.224206Z",
     "iopub.status.busy": "2025-09-29T10:05:17.223861Z",
     "iopub.status.idle": "2025-09-29T10:18:08.991845Z",
     "shell.execute_reply": "2025-09-29T10:18:08.991296Z",
     "shell.execute_reply.started": "2025-09-29T10:05:17.224182Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'开始timestamp': '2025-09-29 18:05:17'}\n",
      "总请求数: 4671, 线程池大小: 200\n",
      "\n",
      "处理批次: 1 - 200/4671\n",
      "批次 1-200 结果已保存\n",
      "\n",
      "处理批次: 201 - 400/4671\n",
      "批次 201-400 结果已保存\n",
      "\n",
      "处理批次: 401 - 600/4671\n",
      "批次 401-600 结果已保存\n",
      "\n",
      "处理批次: 601 - 800/4671\n",
      "批次 601-800 结果已保存\n",
      "\n",
      "处理批次: 801 - 1000/4671\n",
      "批次 801-1000 结果已保存\n",
      "\n",
      "处理批次: 1001 - 1200/4671\n",
      "批次 1001-1200 结果已保存\n",
      "\n",
      "处理批次: 1201 - 1400/4671\n",
      "批次 1201-1400 结果已保存\n",
      "\n",
      "处理批次: 1401 - 1600/4671\n",
      "批次 1401-1600 结果已保存\n",
      "\n",
      "处理批次: 1601 - 1800/4671\n",
      "批次 1601-1800 结果已保存\n",
      "\n",
      "处理批次: 1801 - 2000/4671\n",
      "批次 1801-2000 结果已保存\n",
      "\n",
      "处理批次: 2001 - 2200/4671\n",
      "批次 2001-2200 结果已保存\n",
      "\n",
      "处理批次: 2201 - 2400/4671\n",
      "批次 2201-2400 结果已保存\n",
      "\n",
      "处理批次: 2401 - 2600/4671\n",
      "批次 2401-2600 结果已保存\n",
      "\n",
      "处理批次: 2601 - 2800/4671\n",
      "批次 2601-2800 结果已保存\n",
      "\n",
      "处理批次: 2801 - 3000/4671\n",
      "批次 2801-3000 结果已保存\n",
      "\n",
      "处理批次: 3001 - 3200/4671\n",
      "批次 3001-3200 结果已保存\n",
      "\n",
      "处理批次: 3201 - 3400/4671\n",
      "批次 3201-3400 结果已保存\n",
      "\n",
      "处理批次: 3401 - 3600/4671\n",
      "批次 3401-3600 结果已保存\n",
      "\n",
      "处理批次: 3601 - 3800/4671\n",
      "批次 3601-3800 结果已保存\n",
      "\n",
      "处理批次: 3801 - 4000/4671\n",
      "批次 3801-4000 结果已保存\n",
      "\n",
      "处理批次: 4001 - 4200/4671\n",
      "批次 4001-4200 结果已保存\n",
      "\n",
      "处理批次: 4201 - 4400/4671\n",
      "批次 4201-4400 结果已保存\n",
      "\n",
      "处理批次: 4401 - 4600/4671\n",
      "批次 4401-4600 结果已保存\n",
      "\n",
      "处理批次: 4601 - 4671/4671\n",
      "批次 4601-4671 结果已保存\n",
      "\n",
      "所有请求处理完成，最终结果已保存到 ./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-test_result.json\n",
      "{'结束timestamp': '2025-09-29 18:18:08'}\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift deploy \\\n",
    "    --adapters lora=./train_ada/v3-20250928-095853/checkpoint-2300 \\\n",
    "    --infer_backend vllm \\\n",
    "    --temperature 0 \\\n",
    "    --max_new_tokens 2048 \\\n",
    "    --served_model_name Qwen2.5-7B-Instruct-lora1\n",
    "    \n",
    "'''\n",
    "content = '现在你是一个肿瘤学科医生，请根据患者的问题给出实际的医疗建议：'\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests  \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def call_llm_api(prompt, api_url):\n",
    "    \"\"\"调用大模型API\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        # \"model\": \"Qwen2.5-7B-Instruct\",\n",
    "        \"model\": \"lora\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        # 从聊天接口正确解析结果（chat completions返回的是message而非text）\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"API调用错误: {str(e)}\")\n",
    "        # 抛出异常而非返回错误字符串，让上层处理\n",
    "        raise Exception(f\"API调用错误: {str(e)}\")\n",
    "\n",
    "def process_in_batches(data, api_url, output_file=\"./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-test_result.json\"):\n",
    "    total = len(data)\n",
    "    print(f\"总请求数: {total}, 线程池大小: {MAX_WORKERS}\")\n",
    "\n",
    "    results = []\n",
    "    lock = threading.Lock()  # 确保多线程安全写入结果列表\n",
    "\n",
    "    # 初始化线程池\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 分批次处理\n",
    "        for batch_start in range(0, total, MAX_WORKERS):\n",
    "#             if batch_start%400 ==0 or batch_start%300==0:\n",
    "\n",
    "            batch_end = min(batch_start + MAX_WORKERS, total)\n",
    "            print(f\"\\n处理批次: {batch_start+1} - {batch_end}/{total}\")\n",
    "\n",
    "            # 提交当前批次的所有任务到线程池\n",
    "            futures = {}\n",
    "            for i in range(batch_start, batch_end):\n",
    "                item = data[i]['input']\n",
    "\n",
    "                # 修复：确保content变量已定义（这里假设content是你的prompt模板）\n",
    "                # 如果你没有定义content，请替换为实际的prompt构建逻辑\n",
    "                prompt = content+item\n",
    "                future = executor.submit(\n",
    "                    call_llm_api,\n",
    "                    prompt,  # 使用构建好的prompt\n",
    "                    api_url\n",
    "                )\n",
    "                futures[future] = (i, data[i])  # 关联future与请求索引和数据\n",
    "\n",
    "            # 等待当前批次所有任务完成\n",
    "            batch_results = []\n",
    "            for future in as_completed(futures):\n",
    "                i, item = futures[future]\n",
    "                try:\n",
    "                    model_response = future.result()\n",
    "                    batch_results.append({\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": item['output'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                    # print(f\"完成请求 #{i+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"请求 #{i+1} 处理失败: {str(e)}\")\n",
    "                    batch_results.append({\n",
    "                        \"error\": str(e),\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": item['output'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"failed\"\n",
    "                    })\n",
    "                \n",
    "            # 批次完成后合并结果并保存\n",
    "            with lock:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            # 每批处理完成后保存一次结果\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"批次 {batch_start+1}-{batch_end} 结果已保存\")\n",
    "\n",
    "    print(f\"\\n所有请求处理完成，最终结果已保存到 {output_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('../../data_process/final_data/test.json', 'r', encoding='utf-8') as file:\n",
    "        test_json = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "    # 配置参数\n",
    "    MAX_WORKERS = 200 # 线程池最大线程数，同时也是每批处理的请求数\n",
    "    API_URL = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "    print({\"开始timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_in_batches(test_json, API_URL)\n",
    "    print({\"结束timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f9fdd8c-3ce3-4b33-a6b8-a7742e7daf16",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
