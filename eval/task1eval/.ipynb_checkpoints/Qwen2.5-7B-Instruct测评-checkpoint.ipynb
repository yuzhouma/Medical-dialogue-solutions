{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5447e827-bd21-4d7f-8230-7d651b2f3f8d",
   "metadata": {},
   "source": [
    "# CMMLU测评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "d103c6cf-1ce2-4535-beca-8f7518d6dfa1",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.execute_input": "2025-09-29T02:06:48.115981Z",
     "iopub.status.busy": "2025-09-29T02:06:48.115762Z",
     "iopub.status.idle": "2025-09-29T02:09:26.658975Z",
     "shell.execute_reply": "2025-09-29T02:09:26.658341Z",
     "shell.execute_reply.started": "2025-09-29T02:06:48.115966Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'开始timestamp': '2025-09-29 10:06:48'}\n",
      "总请求数: 11582, 线程池大小: 200\n",
      "\n",
      "处理批次: 1 - 200/11582\n",
      "批次 1-200 结果已保存\n",
      "\n",
      "处理批次: 201 - 400/11582\n",
      "批次 201-400 结果已保存\n",
      "\n",
      "处理批次: 401 - 600/11582\n",
      "批次 401-600 结果已保存\n",
      "\n",
      "处理批次: 601 - 800/11582\n",
      "批次 601-800 结果已保存\n",
      "\n",
      "处理批次: 801 - 1000/11582\n",
      "批次 801-1000 结果已保存\n",
      "\n",
      "处理批次: 1001 - 1200/11582\n",
      "批次 1001-1200 结果已保存\n",
      "\n",
      "处理批次: 1201 - 1400/11582\n",
      "批次 1201-1400 结果已保存\n",
      "\n",
      "处理批次: 1401 - 1600/11582\n",
      "批次 1401-1600 结果已保存\n",
      "\n",
      "处理批次: 1601 - 1800/11582\n",
      "批次 1601-1800 结果已保存\n",
      "\n",
      "处理批次: 1801 - 2000/11582\n",
      "批次 1801-2000 结果已保存\n",
      "\n",
      "处理批次: 2001 - 2200/11582\n",
      "批次 2001-2200 结果已保存\n",
      "\n",
      "处理批次: 2201 - 2400/11582\n",
      "批次 2201-2400 结果已保存\n",
      "\n",
      "处理批次: 2401 - 2600/11582\n",
      "批次 2401-2600 结果已保存\n",
      "\n",
      "处理批次: 2601 - 2800/11582\n",
      "批次 2601-2800 结果已保存\n",
      "\n",
      "处理批次: 2801 - 3000/11582\n",
      "批次 2801-3000 结果已保存\n",
      "\n",
      "处理批次: 3001 - 3200/11582\n",
      "批次 3001-3200 结果已保存\n",
      "\n",
      "处理批次: 3201 - 3400/11582\n",
      "批次 3201-3400 结果已保存\n",
      "\n",
      "处理批次: 3401 - 3600/11582\n",
      "批次 3401-3600 结果已保存\n",
      "\n",
      "处理批次: 3601 - 3800/11582\n",
      "批次 3601-3800 结果已保存\n",
      "\n",
      "处理批次: 3801 - 4000/11582\n",
      "批次 3801-4000 结果已保存\n",
      "\n",
      "处理批次: 4001 - 4200/11582\n",
      "批次 4001-4200 结果已保存\n",
      "\n",
      "处理批次: 4201 - 4400/11582\n",
      "批次 4201-4400 结果已保存\n",
      "\n",
      "处理批次: 4401 - 4600/11582\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 88\u001b[39m, in \u001b[36mprocess_in_batches\u001b[39m\u001b[34m(data, api_url, output_file)\u001b[39m\n\u001b[32m     87\u001b[39m batch_results = []\n\u001b[32m---> \u001b[39m\u001b[32m88\u001b[39m \u001b[43m\u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mas_completed\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     89\u001b[39m \u001b[43m    \u001b[49m\u001b[43mi\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mitem\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[43mfutures\u001b[49m\u001b[43m[\u001b[49m\u001b[43mfuture\u001b[49m\u001b[43m]\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/concurrent/futures/_base.py:243\u001b[39m, in \u001b[36mas_completed\u001b[39m\u001b[34m(fs, timeout)\u001b[39m\n\u001b[32m    239\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTimeoutError\u001b[39;00m(\n\u001b[32m    240\u001b[39m                 \u001b[33m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m (of \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m) futures unfinished\u001b[39m\u001b[33m'\u001b[39m % (\n\u001b[32m    241\u001b[39m                 \u001b[38;5;28mlen\u001b[39m(pending), total_futures))\n\u001b[32m--> \u001b[39m\u001b[32m243\u001b[39m \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43mevent\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait_timeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    245\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m waiter.lock:\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/threading.py:629\u001b[39m, in \u001b[36mEvent.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    628\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m signaled:\n\u001b[32m--> \u001b[39m\u001b[32m629\u001b[39m     signaled = \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_cond\u001b[49m\u001b[43m.\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    630\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m signaled\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/threading.py:327\u001b[39m, in \u001b[36mCondition.wait\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m    326\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m327\u001b[39m     \u001b[43mwaiter\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    328\u001b[39m     gotit = \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: ",
      "\nDuring handling of the above exception, another exception occurred:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 178\u001b[39m\n\u001b[32m    176\u001b[39m API_URL = \u001b[33m\"\u001b[39m\u001b[33mhttp://0.0.0.0:8000/v1/chat/completions\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    177\u001b[39m \u001b[38;5;28mprint\u001b[39m({\u001b[33m\"\u001b[39m\u001b[33m开始timestamp\u001b[39m\u001b[33m\"\u001b[39m: datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)})\n\u001b[32m--> \u001b[39m\u001b[32m178\u001b[39m \u001b[43mprocess_in_batches\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcmmlu_eval_json\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mAPI_URL\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    179\u001b[39m \u001b[38;5;28mprint\u001b[39m({\u001b[33m\"\u001b[39m\u001b[33m结束timestamp\u001b[39m\u001b[33m\"\u001b[39m: datetime.now().strftime(\u001b[33m\"\u001b[39m\u001b[33m%\u001b[39m\u001b[33mY-\u001b[39m\u001b[33m%\u001b[39m\u001b[33mm-\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m \u001b[39m\u001b[33m%\u001b[39m\u001b[33mH:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mM:\u001b[39m\u001b[33m%\u001b[39m\u001b[33mS\u001b[39m\u001b[33m\"\u001b[39m)})\n\u001b[32m    180\u001b[39m process_json_file(\u001b[33m'\u001b[39m\u001b[33m./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-cmmlu_result.json\u001b[39m\u001b[33m'\u001b[39m)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[15]\u001b[39m\u001b[32m, line 63\u001b[39m, in \u001b[36mprocess_in_batches\u001b[39m\u001b[34m(data, api_url, output_file)\u001b[39m\n\u001b[32m     60\u001b[39m     lock = threading.Lock()  \u001b[38;5;66;03m# 确保多线程安全写入结果列表\u001b[39;00m\n\u001b[32m     62\u001b[39m     \u001b[38;5;66;03m# 初始化线程池\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m63\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43;01mwith\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mThreadPoolExecutor\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmax_workers\u001b[49m\u001b[43m=\u001b[49m\u001b[43mMAX_WORKERS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mas\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mexecutor\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     64\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;66;43;03m# 分批次处理\u001b[39;49;00m\n\u001b[32m     65\u001b[39m \u001b[43m        \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43mrange\u001b[39;49m\u001b[43m(\u001b[49m\u001b[32;43m0\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_WORKERS\u001b[49m\u001b[43m)\u001b[49m\u001b[43m:\u001b[49m\n\u001b[32m     66\u001b[39m \u001b[38;5;66;43;03m#             if batch_start%400 ==0 or batch_start%300==0:\u001b[39;49;00m\n\u001b[32m     68\u001b[39m \u001b[43m            \u001b[49m\u001b[43mbatch_end\u001b[49m\u001b[43m \u001b[49m\u001b[43m=\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mmin\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mbatch_start\u001b[49m\u001b[43m \u001b[49m\u001b[43m+\u001b[49m\u001b[43m \u001b[49m\u001b[43mMAX_WORKERS\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtotal\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/concurrent/futures/_base.py:647\u001b[39m, in \u001b[36mExecutor.__exit__\u001b[39m\u001b[34m(self, exc_type, exc_val, exc_tb)\u001b[39m\n\u001b[32m    646\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, exc_type, exc_val, exc_tb):\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mshutdown\u001b[49m\u001b[43m(\u001b[49m\u001b[43mwait\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n\u001b[32m    648\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/concurrent/futures/thread.py:235\u001b[39m, in \u001b[36mThreadPoolExecutor.shutdown\u001b[39m\u001b[34m(self, wait, cancel_futures)\u001b[39m\n\u001b[32m    233\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m wait:\n\u001b[32m    234\u001b[39m     \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m._threads:\n\u001b[32m--> \u001b[39m\u001b[32m235\u001b[39m         \u001b[43mt\u001b[49m\u001b[43m.\u001b[49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/threading.py:1119\u001b[39m, in \u001b[36mThread.join\u001b[39m\u001b[34m(self, timeout)\u001b[39m\n\u001b[32m   1116\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mRuntimeError\u001b[39;00m(\u001b[33m\"\u001b[39m\u001b[33mcannot join current thread\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m   1118\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m timeout \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1119\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_wait_for_tstate_lock\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1120\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m   1121\u001b[39m     \u001b[38;5;66;03m# the behavior of a negative timeout isn't documented, but\u001b[39;00m\n\u001b[32m   1122\u001b[39m     \u001b[38;5;66;03m# historically .join(timeout=x) for x<0 has acted as if timeout=0\u001b[39;00m\n\u001b[32m   1123\u001b[39m     \u001b[38;5;28mself\u001b[39m._wait_for_tstate_lock(timeout=\u001b[38;5;28mmax\u001b[39m(timeout, \u001b[32m0\u001b[39m))\n",
      "\u001b[36mFile \u001b[39m\u001b[32m/usr/local/lib/python3.11/threading.py:1139\u001b[39m, in \u001b[36mThread._wait_for_tstate_lock\u001b[39m\u001b[34m(self, block, timeout)\u001b[39m\n\u001b[32m   1136\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m   1138\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m1139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[43mlock\u001b[49m\u001b[43m.\u001b[49m\u001b[43macquire\u001b[49m\u001b[43m(\u001b[49m\u001b[43mblock\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtimeout\u001b[49m\u001b[43m)\u001b[49m:\n\u001b[32m   1140\u001b[39m         lock.release()\n\u001b[32m   1141\u001b[39m         \u001b[38;5;28mself\u001b[39m._stop()\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 swift deploy \\\n",
    "    --model Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --infer_backend vllm \\\n",
    "    --max_new_tokens 2048 \\\n",
    "    --served_model_name Qwen2.5-7B-Instruct\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import json\n",
    "import io\n",
    "import sys\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests  \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def call_llm_api(prompt, api_url):\n",
    "    \"\"\"调用大模型API\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"Qwen2.5-7B-Instruct\",\n",
    "        # \"model\": \"lora\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        # 从聊天接口正确解析结果（chat completions返回的是message而非text）\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"API调用错误: {str(e)}\")\n",
    "        # 抛出异常而非返回错误字符串，让上层处理\n",
    "        raise Exception(f\"API调用错误: {str(e)}\")\n",
    "\n",
    "def process_in_batches(data, api_url, output_file=\"./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-cmmlu_result.json\"):\n",
    "    total = len(data)\n",
    "    print(f\"总请求数: {total}, 线程池大小: {MAX_WORKERS}\")\n",
    "\n",
    "    results = []\n",
    "    lock = threading.Lock()  # 确保多线程安全写入结果列表\n",
    "\n",
    "    # 初始化线程池\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 分批次处理\n",
    "        for batch_start in range(0, total, MAX_WORKERS):\n",
    "#             if batch_start%400 ==0 or batch_start%300==0:\n",
    "\n",
    "            batch_end = min(batch_start + MAX_WORKERS, total)\n",
    "            print(f\"\\n处理批次: {batch_start+1} - {batch_end}/{total}\")\n",
    "\n",
    "            # 提交当前批次的所有任务到线程池\n",
    "            futures = {}\n",
    "            for i in range(batch_start, batch_end):\n",
    "                item = data[i]['question_with_options']\n",
    "\n",
    "                # 修复：确保content变量已定义（这里假设content是你的prompt模板）\n",
    "                # 如果你没有定义content，请替换为实际的prompt构建逻辑\n",
    "                prompt = content+item\n",
    "                future = executor.submit(\n",
    "                    call_llm_api,\n",
    "                    prompt,  # 使用构建好的prompt\n",
    "                    api_url\n",
    "                )\n",
    "                futures[future] = (i, data[i])  # 关联future与请求索引和数据\n",
    "\n",
    "            # 等待当前批次所有任务完成\n",
    "            batch_results = []\n",
    "            for future in as_completed(futures):\n",
    "                i, item = futures[future]\n",
    "                try:\n",
    "                    model_response = future.result()\n",
    "                    batch_results.append({\n",
    "                        \"question_with_options\": item['question_with_options'],\n",
    "                        \"answer\": item['answer'],\n",
    "                        \"source_file\":item['source_file'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                    # print(f\"完成请求 #{i+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"请求 #{i+1} 处理失败: {str(e)}\")\n",
    "                    batch_results.append({\n",
    "                        \"error\": str(e),\n",
    "                        \"question_with_options\": item['question_with_options'],\n",
    "                        \"answer\": item['answer'],\n",
    "                        \"source_file\":item['source_file'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"failed\"\n",
    "                    })\n",
    "                \n",
    "            # 批次完成后合并结果并保存\n",
    "            with lock:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            # 每批处理完成后保存一次结果\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"批次 {batch_start+1}-{batch_end} 结果已保存\")\n",
    "\n",
    "    print(f\"\\n所有请求处理完成，最终结果已保存到 {output_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_json_file(input_file, output_file='./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-eval_results.xlsx'):\n",
    "    \"\"\"\n",
    "    直接处理JSON文件并生成结果\n",
    "    \"\"\"\n",
    "    # 读取JSON文件\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 按source_file分组统计\n",
    "    stats = {}\n",
    "    for item in data:\n",
    "        source_file = item['source_file']\n",
    "        if source_file not in stats:\n",
    "            stats[source_file] = {'correct': 0, 'total': 0}\n",
    "        \n",
    "        stats[source_file]['total'] += 1\n",
    "        if item['answer'] == item['model_response']:\n",
    "            stats[source_file]['correct'] += 1\n",
    "    \n",
    "    # 准备结果数据\n",
    "    results = []\n",
    "    for source_file, stat in stats.items():\n",
    "        acc = stat['correct'] / stat['total'] if stat['total'] > 0 else 0\n",
    "        results.append({\n",
    "            '源文件': source_file,\n",
    "            '正确数': stat['correct'],\n",
    "            '总数': stat['total'], \n",
    "            '准确率': round(acc, 4)\n",
    "        })\n",
    "    \n",
    "    # 保存到Excel\n",
    "    df = pd.DataFrame(results)\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='cmmlu各个子集acc结果', index=False)\n",
    "    \n",
    "    print(f\"处理完成！共处理 {len(data)} 条数据，生成 {len(results)} 个统计结果\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cmmlu_eval_df = pd.read_csv('cmmlu_concat.csv')\n",
    "    cmmlu_eval_json = cmmlu_eval_df.to_json(orient='records', force_ascii=False, indent=2)\n",
    "    cmmlu_eval_json = json.loads(cmmlu_eval_json)\n",
    "\n",
    "    with open('../../prompt/CMMLU评估', 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "\n",
    "\n",
    "    # 配置参数\n",
    "    MAX_WORKERS = 200 # 线程池最大线程数，同时也是每批处理的请求数\n",
    "    API_URL = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "    print({\"开始timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_in_batches(cmmlu_eval_json, API_URL)\n",
    "    print({\"结束timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_json_file('./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-cmmlu_result.json')\n",
    "    print({\"统计结果已保存\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05dbae9b-919a-4cf3-9e29-9c641dbe0f01",
   "metadata": {},
   "source": [
    "# 测试集测评"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19902fc4-9c0b-492f-9d15-3a141811304b",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2025-09-29T01:54:51.055026Z",
     "iopub.status.idle": "2025-09-29T01:54:51.055280Z",
     "shell.execute_reply": "2025-09-29T01:54:51.055168Z",
     "shell.execute_reply.started": "2025-09-29T01:54:51.055156Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 swift deploy \\\n",
    "    --model Qwen/Qwen2.5-7B-Instruct \\\n",
    "    --infer_backend vllm \\\n",
    "    --max_new_tokens 2048 \\\n",
    "    --served_model_name Qwen2.5-7B-Instruct\n",
    "'''\n",
    "content = '现在你是一个肿瘤学科医生，请根据患者的问题给出实际的医疗建议：'\n",
    "# import pandas as pd\n",
    "# import io\n",
    "# import sys\n",
    "# import json\n",
    "# import threading\n",
    "# from datetime import datetime\n",
    "# from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "# import requests  \n",
    "# import time\n",
    "\n",
    "\n",
    "\n",
    "def call_llm_api(prompt, api_url):\n",
    "    \"\"\"调用大模型API\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        \"model\": \"Qwen2.5-7B-Instruct\",\n",
    "        # \"model\": \"lora\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 1024,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        # 从聊天接口正确解析结果（chat completions返回的是message而非text）\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"API调用错误: {str(e)}\")\n",
    "        # 抛出异常而非返回错误字符串，让上层处理\n",
    "        raise Exception(f\"API调用错误: {str(e)}\")\n",
    "\n",
    "def process_in_batches(data, api_url, output_file=\"./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-test_result.json\"):\n",
    "    total = len(data)\n",
    "    print(f\"总请求数: {total}, 线程池大小: {MAX_WORKERS}\")\n",
    "\n",
    "    results = []\n",
    "    lock = threading.Lock()  # 确保多线程安全写入结果列表\n",
    "\n",
    "    # 初始化线程池\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 分批次处理\n",
    "        for batch_start in range(0, total, MAX_WORKERS):\n",
    "#             if batch_start%400 ==0 or batch_start%300==0:\n",
    "\n",
    "            batch_end = min(batch_start + MAX_WORKERS, total)\n",
    "            print(f\"\\n处理批次: {batch_start+1} - {batch_end}/{total}\")\n",
    "\n",
    "            # 提交当前批次的所有任务到线程池\n",
    "            futures = {}\n",
    "            for i in range(batch_start, batch_end):\n",
    "                item = data[i]['input']\n",
    "\n",
    "                # 修复：确保content变量已定义（这里假设content是你的prompt模板）\n",
    "                # 如果你没有定义content，请替换为实际的prompt构建逻辑\n",
    "                prompt = content+item\n",
    "                future = executor.submit(\n",
    "                    call_llm_api,\n",
    "                    prompt,  # 使用构建好的prompt\n",
    "                    api_url\n",
    "                )\n",
    "                futures[future] = (i, data[i])  # 关联future与请求索引和数据\n",
    "\n",
    "            # 等待当前批次所有任务完成\n",
    "            batch_results = []\n",
    "            for future in as_completed(futures):\n",
    "                i, item = futures[future]\n",
    "                try:\n",
    "                    model_response = future.result()\n",
    "                    batch_results.append({\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": item['output'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                    # print(f\"完成请求 #{i+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"请求 #{i+1} 处理失败: {str(e)}\")\n",
    "                    batch_results.append({\n",
    "                        \"error\": str(e),\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": item['output'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"failed\"\n",
    "                    })\n",
    "                \n",
    "            # 批次完成后合并结果并保存\n",
    "            with lock:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            # 每批处理完成后保存一次结果\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"批次 {batch_start+1}-{batch_end} 结果已保存\")\n",
    "\n",
    "    print(f\"\\n所有请求处理完成，最终结果已保存到 {output_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('../../data_process/final_data/test.json', 'r', encoding='utf-8') as file:\n",
    "        test_json = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "    # 配置参数\n",
    "    MAX_WORKERS = 100 # 线程池最大线程数，同时也是每批处理的请求数\n",
    "    API_URL = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "    print({\"开始timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_in_batches(test_json, API_URL)\n",
    "    print({\"结束timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a88b2f4-74fb-416b-889d-459a7aec9eef",
   "metadata": {
    "ExecutionIndicator": {
     "show": true
    },
    "execution": {
     "iopub.status.busy": "2025-09-29T01:32:35.581698Z",
     "iopub.status.idle": "2025-09-29T01:32:35.581981Z",
     "shell.execute_reply": "2025-09-29T01:32:35.581852Z",
     "shell.execute_reply.started": "2025-09-29T01:32:35.581839Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# !pip install bert-score\n",
    "from rouge import Rouge\n",
    "from nltk.translate.bleu_score import sentence_bleu, SmoothingFunction\n",
    "import bert_score\n",
    "import jieba\n",
    "def simple_evaluation(json_file_path, output_excel_path):\n",
    "    \"\"\"简化版本的评估函数\"\"\"\n",
    "    \n",
    "    # 读取数据\n",
    "    with open(json_file_path, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    rouge = Rouge()\n",
    "    results = []\n",
    "    \n",
    "    for i, item in enumerate(data):\n",
    "        ref = item.get('output', '')\n",
    "        cand = item.get('model_response', '')\n",
    "        \n",
    "        if not ref or not cand:\n",
    "            continue\n",
    "            \n",
    "        # 分词\n",
    "        ref_tokens = list(jieba.cut(ref))\n",
    "        cand_tokens = list(jieba.cut(cand))\n",
    "        \n",
    "        # 计算指标\n",
    "        try:\n",
    "            # ROUGE\n",
    "            rouge_scores = rouge.get_scores(' '.join(cand_tokens), ' '.join(ref_tokens))[0]\n",
    "            \n",
    "            # BLEU\n",
    "            smooth = SmoothingFunction().method1\n",
    "            bleu_4 = sentence_bleu([ref_tokens], cand_tokens, weights=(0.25,0.25,0.25,0.25), smoothing_function=smooth)\n",
    "            \n",
    "            # BERT Score\n",
    "            P, R, F1 = bert_score.score([cand], [ref], lang='zh', verbose=False)\n",
    "            \n",
    "            results.append({\n",
    "                'index': i,\n",
    "                'input': item.get('input', ''),\n",
    "                'rouge_2': rouge_scores['rouge-2']['f'],\n",
    "                'rouge_l': rouge_scores['rouge-l']['f'],\n",
    "                'bleu_4': bleu_4,\n",
    "                'bert_score_f1': F1.item()\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"计算第 {i} 条数据时出错: {e}\")\n",
    "            continue\n",
    "    \n",
    "    # 保存结果\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(output_excel_path, index=False)\n",
    "    return df\n",
    "\n",
    "\n",
    "df = simple_evaluation(\"./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-test_result.json\", \"./all_model_output/Qwen2.5-7B-Instruct/Qwen2.5-7B-Instruct-test_result.xlsx\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "124a1329-3473-4807-81a6-815fa4ff32a9",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
