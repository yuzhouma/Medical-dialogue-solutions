{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "d103c6cf-1ce2-4535-beca-8f7518d6dfa1",
   "metadata": {
    "ExecutionIndicator": {
     "show": false
    },
    "execution": {
     "iopub.execute_input": "2025-09-29T09:54:25.369540Z",
     "iopub.status.busy": "2025-09-29T09:54:25.369342Z",
     "iopub.status.idle": "2025-09-29T09:54:43.071638Z",
     "shell.execute_reply": "2025-09-29T09:54:43.071045Z",
     "shell.execute_reply.started": "2025-09-29T09:54:25.369524Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'开始timestamp': '2025-09-29 17:54:25'}\n",
      "总请求数: 11582, 线程池大小: 250\n",
      "\n",
      "处理批次: 1 - 250/11582\n",
      "批次 1-250 结果已保存\n",
      "\n",
      "处理批次: 251 - 500/11582\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "KeyboardInterrupt\n",
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "\n",
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift deploy \\\n",
    "    --adapters lora=./train_ada/v3-20250928-095853/checkpoint-2300 \\\n",
    "    --infer_backend vllm \\\n",
    "    --temperature 0 \\\n",
    "    --max_new_tokens 2048 \\\n",
    "    --served_model_name Qwen2.5-7B-Instruct-lora1\n",
    "'''\n",
    "\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import io\n",
    "import sys\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests  # 需安装：pip install requests\n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def call_llm_api(prompt, api_url):\n",
    "    \"\"\"调用大模型API\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        # \"model\": \"Qwen2.5-7B-Instruct\",\n",
    "        \"model\": \"lora\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 50,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        # 从聊天接口正确解析结果（chat completions返回的是message而非text）\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"API调用错误: {str(e)}\")\n",
    "        # 抛出异常而非返回错误字符串，让上层处理\n",
    "        raise Exception(f\"API调用错误: {str(e)}\")\n",
    "\n",
    "def process_in_batches(data, api_url, output_file=\"./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-cmmlu_result.json\"):\n",
    "    total = len(data)\n",
    "    print(f\"总请求数: {total}, 线程池大小: {MAX_WORKERS}\")\n",
    "\n",
    "    results = []\n",
    "    lock = threading.Lock()  # 确保多线程安全写入结果列表\n",
    "\n",
    "    # 初始化线程池\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 分批次处理\n",
    "        for batch_start in range(0, total, MAX_WORKERS):\n",
    "#             if batch_start%400 ==0 or batch_start%300==0:\n",
    "\n",
    "            batch_end = min(batch_start + MAX_WORKERS, total)\n",
    "            print(f\"\\n处理批次: {batch_start+1} - {batch_end}/{total}\")\n",
    "\n",
    "            # 提交当前批次的所有任务到线程池\n",
    "            futures = {}\n",
    "            for i in range(batch_start, batch_end):\n",
    "                item = data[i]['question_with_options']\n",
    "\n",
    "                # 修复：确保content变量已定义（这里假设content是你的prompt模板）\n",
    "                # 如果你没有定义content，请替换为实际的prompt构建逻辑\n",
    "                prompt = content+item\n",
    "                future = executor.submit(\n",
    "                    call_llm_api,\n",
    "                    prompt,  # 使用构建好的prompt\n",
    "                    api_url\n",
    "                )\n",
    "                futures[future] = (i, data[i])  # 关联future与请求索引和数据\n",
    "\n",
    "            # 等待当前批次所有任务完成\n",
    "            batch_results = []\n",
    "            for future in as_completed(futures):\n",
    "                i, item = futures[future]\n",
    "                try:\n",
    "                    model_response = future.result()\n",
    "                    batch_results.append({\n",
    "                        \"question_with_options\": item['question_with_options'],\n",
    "                        \"answer\": item['answer'],\n",
    "                        \"source_file\":item['source_file'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                    # print(f\"完成请求 #{i+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"请求 #{i+1} 处理失败: {str(e)}\")\n",
    "                    batch_results.append({\n",
    "                        \"error\": str(e),\n",
    "                        \"question_with_options\": item['question_with_options'],\n",
    "                        \"answer\": item['answer'],\n",
    "                        \"source_file\":item['source_file'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"failed\"\n",
    "                    })\n",
    "                \n",
    "            # 批次完成后合并结果并保存\n",
    "            with lock:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            # 每批处理完成后保存一次结果\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"批次 {batch_start+1}-{batch_end} 结果已保存\")\n",
    "\n",
    "    print(f\"\\n所有请求处理完成，最终结果已保存到 {output_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "def process_json_file(input_file, output_file='./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-eval_results.xlsx'):\n",
    "    \"\"\"\n",
    "    直接处理JSON文件并生成结果\n",
    "    \"\"\"\n",
    "    # 读取JSON文件\n",
    "    with open(input_file, 'r', encoding='utf-8') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    # 按source_file分组统计\n",
    "    stats = {}\n",
    "    for item in data:\n",
    "        source_file = item['source_file']\n",
    "        if source_file not in stats:\n",
    "            stats[source_file] = {'correct': 0, 'total': 0}\n",
    "        \n",
    "        stats[source_file]['total'] += 1\n",
    "        if item['answer'] == item['model_response']:\n",
    "            stats[source_file]['correct'] += 1\n",
    "    \n",
    "    # 准备结果数据\n",
    "    results = []\n",
    "    for source_file, stat in stats.items():\n",
    "        acc = stat['correct'] / stat['total'] if stat['total'] > 0 else 0\n",
    "        results.append({\n",
    "            '源文件': source_file,\n",
    "            '正确数': stat['correct'],\n",
    "            '总数': stat['total'], \n",
    "            '准确率': round(acc, 4)\n",
    "        })\n",
    "    \n",
    "    # 保存到Excel\n",
    "    df = pd.DataFrame(results)\n",
    "    with pd.ExcelWriter(output_file, engine='openpyxl') as writer:\n",
    "        df.to_excel(writer, sheet_name='cmmlu各个子集acc结果', index=False)\n",
    "    \n",
    "    print(f\"处理完成！共处理 {len(data)} 条数据，生成 {len(results)} 个统计结果\")\n",
    "    return df\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    cmmlu_eval_df = pd.read_csv('cmmlu_concat.csv')\n",
    "    cmmlu_eval_json = cmmlu_eval_df.to_json(orient='records', force_ascii=False, indent=2)\n",
    "    cmmlu_eval_json = json.loads(cmmlu_eval_json)\n",
    "\n",
    "    with open('../../prompt/CMMLU评估', 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "\n",
    "\n",
    "\n",
    "    # 配置参数\n",
    "    MAX_WORKERS = 250 # 线程池最大线程数，同时也是每批处理的请求数\n",
    "    API_URL = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "    print({\"开始timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_in_batches(cmmlu_eval_json, API_URL)\n",
    "    print({\"结束timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_json_file('./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-cmmlu_result.json')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19902fc4-9c0b-492f-9d15-3a141811304b",
   "metadata": {},
   "outputs": [],
   "source": [
    "'''\n",
    "CUDA_VISIBLE_DEVICES=0 \\\n",
    "swift deploy \\\n",
    "    --adapters lora=./train_ada/v3-20250928-095853/checkpoint-2300 \\\n",
    "    --infer_backend vllm \\\n",
    "    --temperature 0 \\\n",
    "    --max_new_tokens 2048 \\\n",
    "    --served_model_name Qwen2.5-7B-Instruct-lora1\n",
    "    \n",
    "'''\n",
    "content = '现在你是一个肿瘤学科医生，请根据患者的问题给出实际的医疗建议：'\n",
    "import pandas as pd\n",
    "import io\n",
    "import sys\n",
    "import json\n",
    "import threading\n",
    "from datetime import datetime\n",
    "from concurrent.futures import ThreadPoolExecutor, as_completed\n",
    "import requests  \n",
    "import time\n",
    "\n",
    "\n",
    "\n",
    "def call_llm_api(prompt, api_url):\n",
    "    \"\"\"调用大模型API\"\"\"\n",
    "    headers = {\n",
    "        \"Content-Type\": \"application/json\"\n",
    "    }\n",
    "\n",
    "    payload = {\n",
    "        # \"model\": \"Qwen2.5-7B-Instruct\",\n",
    "        \"model\": \"lora\",\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ],\n",
    "        \"max_tokens\": 512,\n",
    "        \"temperature\": 0.1\n",
    "    }\n",
    "\n",
    "    try:\n",
    "        response = requests.post(\n",
    "            api_url,\n",
    "            headers=headers,\n",
    "            json=payload,\n",
    "            timeout=120\n",
    "        )\n",
    "        response.raise_for_status()\n",
    "        # 从聊天接口正确解析结果（chat completions返回的是message而非text）\n",
    "        return response.json().get(\"choices\", [{}])[0].get(\"message\", {}).get(\"content\", \"\").strip()\n",
    "    except Exception as e:\n",
    "        print(f\"API调用错误: {str(e)}\")\n",
    "        # 抛出异常而非返回错误字符串，让上层处理\n",
    "        raise Exception(f\"API调用错误: {str(e)}\")\n",
    "\n",
    "def process_in_batches(data, api_url, output_file=\"./all_model_output/Qwen2.5-7B-Instruct-lora1/Qwen2.5-7B-Instruct-lora1-test_result.json\"):\n",
    "    total = len(data)\n",
    "    print(f\"总请求数: {total}, 线程池大小: {MAX_WORKERS}\")\n",
    "\n",
    "    results = []\n",
    "    lock = threading.Lock()  # 确保多线程安全写入结果列表\n",
    "\n",
    "    # 初始化线程池\n",
    "    with ThreadPoolExecutor(max_workers=MAX_WORKERS) as executor:\n",
    "        # 分批次处理\n",
    "        for batch_start in range(0, total, MAX_WORKERS):\n",
    "#             if batch_start%400 ==0 or batch_start%300==0:\n",
    "\n",
    "            batch_end = min(batch_start + MAX_WORKERS, total)\n",
    "            print(f\"\\n处理批次: {batch_start+1} - {batch_end}/{total}\")\n",
    "\n",
    "            # 提交当前批次的所有任务到线程池\n",
    "            futures = {}\n",
    "            for i in range(batch_start, batch_end):\n",
    "                item = data[i]['input']\n",
    "\n",
    "                # 修复：确保content变量已定义（这里假设content是你的prompt模板）\n",
    "                # 如果你没有定义content，请替换为实际的prompt构建逻辑\n",
    "                prompt = content+item\n",
    "                future = executor.submit(\n",
    "                    call_llm_api,\n",
    "                    prompt,  # 使用构建好的prompt\n",
    "                    api_url\n",
    "                )\n",
    "                futures[future] = (i, data[i])  # 关联future与请求索引和数据\n",
    "\n",
    "            # 等待当前批次所有任务完成\n",
    "            batch_results = []\n",
    "            for future in as_completed(futures):\n",
    "                i, item = futures[future]\n",
    "                try:\n",
    "                    model_response = future.result()\n",
    "                    batch_results.append({\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": item['output'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"success\"\n",
    "                    })\n",
    "                    # print(f\"完成请求 #{i+1}\")\n",
    "                except Exception as e:\n",
    "                    print(f\"请求 #{i+1} 处理失败: {str(e)}\")\n",
    "                    batch_results.append({\n",
    "                        \"error\": str(e),\n",
    "                        \"input\": item['input'],\n",
    "                        \"output\": item['output'],\n",
    "                        \"model_response\":model_response,\n",
    "                        \"timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\"),\n",
    "                        \"status\": \"failed\"\n",
    "                    })\n",
    "                \n",
    "            # 批次完成后合并结果并保存\n",
    "            with lock:\n",
    "                results.extend(batch_results)\n",
    "\n",
    "            # 每批处理完成后保存一次结果\n",
    "            with open(output_file, 'w', encoding='utf-8') as f:\n",
    "                json.dump(results, f, ensure_ascii=False, indent=2)\n",
    "            print(f\"批次 {batch_start+1}-{batch_end} 结果已保存\")\n",
    "\n",
    "    print(f\"\\n所有请求处理完成，最终结果已保存到 {output_file}\")\n",
    "    return results\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    with open('../../data_process/final_data/test.json', 'r', encoding='utf-8') as file:\n",
    "        test_json = json.load(file)\n",
    "\n",
    "\n",
    "\n",
    "    # 配置参数\n",
    "    MAX_WORKERS = 200 # 线程池最大线程数，同时也是每批处理的请求数\n",
    "    API_URL = \"http://0.0.0.0:8000/v1/chat/completions\"\n",
    "    print({\"开始timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n",
    "    process_in_batches(test_json, API_URL)\n",
    "    print({\"结束timestamp\": datetime.now().strftime(\"%Y-%m-%d %H:%M:%S\")})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
